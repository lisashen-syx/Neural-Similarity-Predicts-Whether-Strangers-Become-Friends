{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom code for manuscript \"Neural Similarity Induces Friendship\"\n",
    "\n",
    "Copyright 2024 Yixuan Shen\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Authors: Yixuan Lisa Shen and Ryan Hyon__\n",
    "\n",
    "__Last updated: 2025/2/26__\n",
    "\n",
    "This is the custom code that was used for the main analyses for the manuscript “Neual Similarity Induces Friendship” under revision at _Nature Human Behavior_. In compliance with the guidelines outlined in the Journal’s Code and Software Submission Checklist, we provide a small and simulated dataset here to demo the code. On a 2021 MacBook Pro with 16GB of RAM, it takes about 10 minutes to run through the code in this demo with the simulated dataset. \n",
    "\n",
    "This code uses simulated data that include 10 participants and their simulated timeseries (100 TRs) in 5 brain regions. There are two input files: \n",
    "1. subject_ts.csv: file that includes simulated timeseries (100 TRs) in 5 brain regions for each of the 10 participants\n",
    "2. subject_demo_ratings.csv: file that includes 10 participants' demographics information (age, gender, nationality, hometown, college, major, and industry) and handedness, as well as their post-scan self-reported enjoyment and interest ratings for each of the 5 videos they have viewed in the scanner. \n",
    "3. dyad_list.csv: file that includes each of the 45 unique dyads from the 10 subjects.\n",
    "4. edgelist_t2.csv: csv file that includes the edgelist at Time 2 arranged in two columns of source and target\n",
    "5. edgelist_t3.csv: csv file that includes the edgelist at Time 3 arranged in two columns of source and target\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in input files (Note: make sure to set the file path to the correct path on your local machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "import igraph\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "filepath = f'/Users/parkinsonlab/Desktop/NHB_friendship/demo_code' #set the file paths to the correct paths on your local machine\n",
    "subject_ts = pd.read_csv(f'{filepath}/subject_ts.csv', index_col=False)\n",
    "subject_demo_ratings = pd.read_csv(f'{filepath}/subject_demo_ratings.csv', index_col=False)\n",
    "dyad = pd.read_csv(f'{filepath}/dyad_list.csv', index_col=False)\n",
    "\n",
    "def SavePickle(infile, outfile):\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(infile, f, protocol = 2)\n",
    "\n",
    "def LoadPickle(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        outfile = pickle.load(f)\n",
    "        return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dyad-level variables and store in dyad-level dataframe (dyad_df): \n",
    "* age_dist: absolute difference in age\n",
    "* gender_similarity: binary indicator of whether two subjects are of the same gender\n",
    "* nationality_similarity: binary indicator of whether two subjects are of the same nationality\n",
    "* hometown_population_similarity: absolute difference in the size of population between two subjects' hometowns\n",
    "* dist_hometown: distance between two subjects' hometowns\n",
    "* dist_college: distance between two subjects' undergraduate alma mater\n",
    "* college_pub_priv_similarity: binary indicator of whether two subjects' undergraduate alma maters are of the same type of institution (public or private)\n",
    "* major_similarity: binary indicator of whether two subjects' undergraduate majors belong to the same category \n",
    "* industry_similarity: binary indicator of whether two subject work in the same industry\n",
    "* handedness_similarity: binary indicator of whether two subjects are of the same handedness\n",
    "* enjoy_similarity: similarity in the enjoyment rating vectors (i.e., Euclidean distance between rating vectors)\n",
    "* interest_similarity: similarity in the interest rating vectors (i.e., Euclidean distance between rating vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyad_df = dyad.copy()\n",
    "\n",
    "dyad_df['age_dist'] =''\n",
    "dyad_df['gender_similarity'] = ''\n",
    "dyad_df['nationality_similarity'] = ''\n",
    "dyad_df['hometown_population_similarity'] = ''\n",
    "dyad_df['dist_hometown'] = ''\n",
    "dyad_df['dist_college'] = ''\n",
    "dyad_df['college_pub_priv_similarity'] = ''\n",
    "dyad_df['major_similarity'] = ''\n",
    "dyad_df['industry_similarity'] = ''\n",
    "dyad_df['handedness_similarity'] = ''\n",
    "dyad_df['interest_similarity'] = ''\n",
    "dyad_df['enjoy_similarity'] = ''\n",
    "\n",
    "#load files that contain demographic information\n",
    "dist_colleges = pd.read_csv(f'{filepath}/demo_info/dist_colleges.csv', index_col=False)\n",
    "dist_hometowns = pd.read_csv(f'{filepath}/demo_info/dist_hometowns.csv', index_col=False)\n",
    "dict_major_cat = LoadPickle(f'{filepath}/demo_info/dict_major_cat.pkl')\n",
    "dict_college_public_private = LoadPickle(f'{filepath}/demo_info/dict_college_public_private.pkl')\n",
    "dict_hometown_population = LoadPickle(f'{filepath}/demo_info/dict_hometown_population.pkl')\n",
    "\n",
    "#get the columns with enjoyment and interest ratings\n",
    "enjoy_cols = [i for i in subject_demo_ratings.columns if 'enjoy' in i]\n",
    "interest_cols = [i for i in subject_demo_ratings.columns if 'interest' in i]\n",
    "\n",
    "for i in range(0,len(dyad_df)):\n",
    "    dyad_subj1 = dyad_df['dyad_subject1'][i]\n",
    "    dyad_subj2 = dyad_df['dyad_subject2'][i]\n",
    "    \n",
    "    #age_dist\n",
    "    dyad_subj1_age = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['age'].item()\n",
    "    dyad_subj2_age = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['age'].item()\n",
    "    age_dist = np.abs(dyad_subj1_age - dyad_subj2_age)\n",
    "    dyad_df['age_dist'][i] = age_dist\n",
    "\n",
    "    #gender_similarity\n",
    "    dyad_subj1_gender = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['gender'].item()\n",
    "    dyad_subj2_gender = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['gender'].item()\n",
    "    gender_similarity = 1 if dyad_subj1_gender == dyad_subj2_gender else 0\n",
    "    dyad_df['gender_similarity'][i] = gender_similarity\n",
    "    \n",
    "    #nationality_similarity\n",
    "    dyad_subj1_nationality = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['nationality'].item()\n",
    "    dyad_subj2_nationality = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['nationality'].item()\n",
    "    nationality_similarity = 1 if dyad_subj1_nationality == dyad_subj2_nationality else 0\n",
    "    dyad_df['nationality_similarity'][i] = nationality_similarity\n",
    "\n",
    "    #hometown_population_similarity\n",
    "    dyad_subj1_hometown = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['hometown'].item()\n",
    "    dyad_subj2_hometown = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['hometown'].item()\n",
    "    hometown_population_similarity = np.abs(dict_hometown_population[dyad_subj1_hometown] - dict_hometown_population[dyad_subj2_hometown])\n",
    "    dyad_df['hometown_population_similarity'][i] = hometown_population_similarity\n",
    "\n",
    "    #dist_hometown\n",
    "    if dyad_subj1_hometown == dyad_subj2_hometown:\n",
    "        dyad_df['dist_hometown'][i] = 0\n",
    "    else: \n",
    "        hometown_idx = dist_hometowns.index[(dist_hometowns['City1'] == dyad_subj1_hometown) & (dist_hometowns['City2'] == dyad_subj2_hometown)].tolist()\n",
    "        if not hometown_idx:\n",
    "            hometown_idx = dist_hometowns.index[(dist_hometowns['City2'] == dyad_subj1_hometown) & (dist_hometowns['City1'] == dyad_subj2_hometown)].tolist()\n",
    "        dyad_df['dist_hometown'][i] = dist_hometowns.loc[hometown_idx[0], 'dist_hometown']\n",
    "\n",
    "    #dist_college\n",
    "    dyad_subj1_college = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['college'].item()\n",
    "    dyad_subj2_college = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['college'].item()\n",
    "\n",
    "    if dyad_subj1_college == dyad_subj2_college:\n",
    "        dyad_df['dist_college'][i] = 0\n",
    "    else:\n",
    "        college_idx = dist_colleges.index[(dist_colleges['college1'] == dyad_subj1_college) & (dist_colleges['college2'] == dyad_subj2_college)].tolist()\n",
    "        if not college_idx:\n",
    "            college_idx = dist_colleges.index[(dist_colleges['college2'] == dyad_subj1_college) & (dist_colleges['college1'] == dyad_subj2_college)].tolist()\n",
    "        dyad_df['dist_college'][i] = dist_colleges.loc[college_idx[0], 'dist_college']\n",
    "\n",
    "    #college_pub_priv_similarity\n",
    "    college_pub_priv_similarity = 1 if dict_college_public_private[dyad_subj1_college] == dict_college_public_private[dyad_subj2_college] else 0\n",
    "    dyad_df['college_pub_priv_similarity'][i] = college_pub_priv_similarity\n",
    "\n",
    "    #major_similarity\n",
    "    dyad_subj1_major = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['major'].item()\n",
    "    dyad_subj2_major = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['major'].item()\n",
    "    major_similarity = 1 if dict_major_cat[dyad_subj1_major] == dict_major_cat[dyad_subj2_major] else 0\n",
    "    dyad_df['major_similarity'][i] = major_similarity\n",
    "\n",
    "    #industry_similarity\n",
    "    dyad_subj1_industry = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['industry'].item()\n",
    "    dyad_subj2_industry = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['industry'].item()\n",
    "    industry_similarity = 1 if dyad_subj1_industry == dyad_subj2_industry else 0\n",
    "    dyad_df['industry_similarity'][i] = industry_similarity\n",
    "\n",
    "    #handedness_similarity\n",
    "    dyad_subj1_handedness = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj1]['handedness'].item()\n",
    "    dyad_subj2_handedness = subject_demo_ratings[subject_demo_ratings['subject']==dyad_subj2]['handedness'].item()\n",
    "    handedness_similarity = 1 if dyad_subj1_handedness == dyad_subj2_handedness else 0\n",
    "    dyad_df['handedness_similarity'][i] = handedness_similarity\n",
    "\n",
    "    #enjoy_similarity\n",
    "    dyad_subj1_enjoy_vec = stats.zscore(subject_demo_ratings[subject_demo_ratings['subject'] == dyad_subj1][enjoy_cols].values.flatten())\n",
    "    dyad_subj2_enjoy_vec = stats.zscore(subject_demo_ratings[subject_demo_ratings['subject'] == dyad_subj2][enjoy_cols].values.flatten())\n",
    "    enjoy_distance = np.linalg.norm(dyad_subj1_enjoy_vec - dyad_subj2_enjoy_vec)\n",
    "    dyad_df['enjoy_similarity'][i] = float(enjoy_distance)\n",
    "   \n",
    "    #interest_similarity\n",
    "    dyad_subj1_interest_vec = stats.zscore(subject_demo_ratings[subject_demo_ratings['subject'] == dyad_subj1][interest_cols].values.flatten())\n",
    "    dyad_subj2_interest_vec = stats.zscore(subject_demo_ratings[subject_demo_ratings['subject'] == dyad_subj2][interest_cols].values.flatten())\n",
    "    interest_distance = np.linalg.norm(dyad_subj1_interest_vec - dyad_subj2_interest_vec)\n",
    "    dyad_df['interest_similarity'][i] = float(interest_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate ISCs in each of the 5 brain regions for each of the 45 unique subject pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = ['brain_region1', 'brain_region2', 'brain_region3', 'brain_region4', 'brain_region5']\n",
    "dyad_df[parcels] = ''\n",
    "\n",
    "for i in range(0,len(dyad_df)):\n",
    "    #get the two subjects in the dyad\n",
    "    dyad_subj1 = dyad_df['dyad_subject1'][i]\n",
    "    dyad_subj2 = dyad_df['dyad_subject2'][i]\n",
    "    \n",
    "    for parcel in parcels:\n",
    "        #get the timeseries for each of the two subjects in the dyad \n",
    "        subj1_index = subject_ts[(subject_ts['subject'] == dyad_subj1) & (subject_ts['brain_parcel'] == parcel)].index.to_list()\n",
    "        subj1_ts = subject_ts.iloc[subj1_index, 2:103].values.flatten()\n",
    "        subj2_index = subject_ts[(subject_ts['subject'] == dyad_subj2) & (subject_ts['brain_parcel'] == parcel)].index.to_list()\n",
    "        subj2_ts = subject_ts.iloc[subj2_index, 2:103].values.flatten()\n",
    "\n",
    "        #calculate their ISCs within a given brain region and apply Fisher-z transformation\n",
    "        dyad_df.loc[i, parcel] = np.arctanh(pearsonr(subj1_ts, subj2_ts)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each brain region, identify disproportionately high and low neural similarity values (i.e., outliers) for values 1.5 times the interquartile range (IQR) above the upper quartile (75th percentile) or below the lower quartile (25th percentile), and replace these outliers with values equal to upper quartile plus 1.5 times the IQR or lower quartile minus the IQR, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_outliers(df):\n",
    "    # recode outliers using IQR method\n",
    "    cols = [i for i in df.columns if 'brain_region' in i]\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(.25)\n",
    "        Q3 = df[col].quantile(.75)\n",
    "        IQR = Q3-Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower = lower_bound, upper = upper_bound)\n",
    "\n",
    "    return df\n",
    "\n",
    "dyad_df = IQR_outliers(dyad_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network graphs using the edgelists and get social distance at Time 2 and Time 3 for each dyad. \\\n",
    "Creaet a master dataframe (master_df) to include all the data needed for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in edgelist files\n",
    "edgelist_t2 = pd.read_csv(f'{filepath}/edgelist_t2.csv', index_col=False)\n",
    "edgelist_t3 = pd.read_csv(f'{filepath}/edgelist_t3.csv', index_col=False)\n",
    "\n",
    "#function to create network graph from edgelist\n",
    "# mode = 'mutual' is specified to make sure a tie exists between 2 nodes only if the friendship is mutually reported (i.e., A nominated B as a friend and B nominated A as a friend)\n",
    "def make_graph(edgelist_time):\n",
    "    g = igraph.Graph.DataFrame(edgelist_time, use_vids=False, directed=True)\n",
    "    g = igraph.Graph.as_undirected(g, mode = 'mutual')\n",
    "\n",
    "    return g\n",
    "\n",
    "#create network graphs for t2 and t3\n",
    "g_t2 = make_graph(edgelist_t2)\n",
    "g_t3 = make_graph(edgelist_t3)\n",
    "\n",
    "#create a master dataframe master_df that includes all the data needed for the analysis. \n",
    "master_df = dyad_df.copy()\n",
    "\n",
    "master_df['soc_dist2'] = ''\n",
    "master_df['soc_dist3'] = ''\n",
    "\n",
    "for i in range(0,len(master_df)):\n",
    "    #get the two subjects in the dyad\n",
    "    dyad_subj1 = dyad_df['dyad_subject1'][i]\n",
    "    dyad_subj2 = dyad_df['dyad_subject2'][i]\n",
    "    \n",
    "    #get the distance between the two subjects in the dyad \n",
    "    master_df.loc[i, 'soc_dist2'] = igraph.Graph.shortest_paths(g_t2, source = dyad_subj1, target = dyad_subj2)[0][0]\n",
    "    master_df.loc[i, 'soc_dist3'] = igraph.Graph.shortest_paths(g_t3, source = dyad_subj1, target = dyad_subj2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permute the social network graph 1000 times at each time point (Time 2 and Time 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up function permute_graph()\n",
    "def permute_graph(subject_ids, n_permutations, time):\n",
    "    for i in range(1, n_permutations+1):\n",
    "        \n",
    "        if time == 't2': \n",
    "            g = g_t2\n",
    "        elif time == 't3':\n",
    "            g = g_t3\n",
    "        else: \n",
    "            print('time input is invalid')\n",
    "\n",
    "        ids = list(subject_ids)\n",
    "        shuffled_ids = random.sample(ids, len(ids))\n",
    "\n",
    "        for id, shuffled_id in zip(ids, shuffled_ids):\n",
    "            index = g.vs['name'].index(id)\n",
    "            g.vs[index]['name'] = shuffled_id\n",
    "\n",
    "        if not os.path.exists(f'{filepath}/derivatives/igraph_data/{time}/permuted_networks/'):\n",
    "            os.makedirs(f'{filepath}/derivatives/igraph_data/{time}/permuted_networks/')\n",
    "        \n",
    "        SavePickle(g, f'{filepath}/derivatives/igraph_data/{time}/permuted_networks/igraph_undirected_mutual_p{i}.pkl')\n",
    "\n",
    "subject_ids = subject_ts['subject'].unique()\n",
    "\n",
    "#permute the social network graph at each timepoint\n",
    "for time in ['t2', 't3']:\n",
    "    permute_graph(subject_ids, n_permutations=1000, time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate master_dfs_permuted based on the permuted graphs. For 1000 permuted graphs, 1000 master_dfs_p{perm}.pkl will be generated. These will be used to create the null models to assess the statistical significance in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soc_dist_pemuted(n_permutations):\n",
    "    for perm in range(1, n_permutations+1):\n",
    "        dyad_df_permuted = dyad_df.copy()\n",
    "        \n",
    "        for i in range(0,len(dyad_df_permuted)):\n",
    "            #get the two subjects in the dyad\n",
    "            dyad_subj1 = dyad_df_permuted['dyad_subject1'][i]\n",
    "            dyad_subj2 = dyad_df_permuted['dyad_subject2'][i]\n",
    "            \n",
    "            for time in ['t2', 't3']:\n",
    "                #read in the permuted network graph\n",
    "                g = LoadPickle(f'{filepath}/derivatives/igraph_data/{time}/permuted_networks/igraph_undirected_mutual_p{perm}.pkl')\n",
    "            \n",
    "                #get the social distances between two nodes in the permuted graph at each timepoint\n",
    "                if time == 't2':\n",
    "                    dyad_df_permuted.loc[i, 'soc_dist2'] = igraph.Graph.shortest_paths(g, source = dyad_subj1, target = dyad_subj2)[0][0]\n",
    "                if time == 't3':\n",
    "                    dyad_df_permuted.loc[i, 'soc_dist3'] = igraph.Graph.shortest_paths(g, source = dyad_subj1, target = dyad_subj2)[0][0]\n",
    "            \n",
    "        if not os.path.exists(f'{filepath}/derivatives/master_dfs_permuted/'):\n",
    "            os.makedirs(f'{filepath}/derivatives/master_dfs_permuted/')\n",
    "        \n",
    "        SavePickle(dyad_df_permuted, f'{filepath}/derivatives/master_dfs_permuted/master_dfs_p{perm}.pkl')\n",
    "\n",
    "#create 1000 master_dfs_p{perm} \n",
    "get_soc_dist_pemuted(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up functions to control for all sociodemographic variables and handedness as well as enjoyment and interest ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to control for sociodemographic variables and handedness\n",
    "def regress_out_covariates(df):\n",
    "    regressor_cols = ['age_dist', 'gender_similarity', 'nationality_similarity', 'hometown_population_similarity', 'dist_hometown','dist_college', 'college_pub_priv_similarity', 'major_similarity', 'industry_similarity', 'handedness_similarity',]\n",
    "    regressors_var = df[regressor_cols]\n",
    "    cols = parcels\n",
    "    df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "    outcome_var = df[cols]\n",
    "\n",
    "    pipe = LinearRegression()\n",
    "    pipe.fit(regressors_var, outcome_var)\n",
    "    predicted = pipe.predict(regressors_var)\n",
    "    actual = outcome_var.values\n",
    "    resid = actual - predicted\n",
    "\n",
    "    resid_df = pd.DataFrame(resid, columns = cols)\n",
    "    df_subset = df[[col for col in df.columns if not col in cols]]\n",
    "    df_final = pd.concat([df_subset, resid_df], axis = 1)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# function to control for enjoyment and interest \n",
    "def regress_out_enjoyment_interest(df):\n",
    "    regressor_cols = ['enjoy_similarity', 'interest_similarity']\n",
    "    regressors_var = df[regressor_cols]\n",
    "    cols = parcels\n",
    "    df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "    outcome_var = df[cols]\n",
    "\n",
    "    pipe = LinearRegression()\n",
    "    pipe.fit(regressors_var, outcome_var)\n",
    "    predicted = pipe.predict(regressors_var)\n",
    "    actual = outcome_var.values\n",
    "    resid = actual - predicted\n",
    "\n",
    "    resid_df = pd.DataFrame(resid, columns = cols)\n",
    "    df_subset = df[[col for col in df.columns if not col in cols]]\n",
    "    df_final = pd.concat([df_subset.reset_index(drop = True), resid_df.reset_index(drop = True)], axis = 1)\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis testing if pre-existing neural similarity differed between levels of social distance at Time 3\n",
    "Setting up functions compare_groups_permuted() and calc_sig() to run the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to run the analysis and calculate statistical significance\n",
    "def compare_groups_permuted(time, contrast, control):\n",
    "    vals = np.zeros([1000, len(parcels)])\n",
    "    for i in range(1000):\n",
    "        df = LoadPickle(f'{filepath}/derivatives/master_dfs_permuted/master_dfs_p{i+1}.pkl')\n",
    "\n",
    "        if control == 'demo':\n",
    "            df = regress_out_covariates(df)\n",
    "        if control == 'enjoyment-interest':\n",
    "            df = regress_out_enjoyment_interest(df)\n",
    "\n",
    "        cols = parcels\n",
    "        df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "\n",
    "        if time == 't2':\n",
    "            soc_dist_col = 'soc_dist2'\n",
    "        elif time == 't3':\n",
    "            soc_dist_col = 'soc_dist3'\n",
    "        else:\n",
    "            print('invalid time input')\n",
    "\n",
    "        for j in range(len(cols)):\n",
    "            col = cols[j]\n",
    "            x = df[df[soc_dist_col].isin([1])][col].values\n",
    "            if contrast == '1v2':\n",
    "                y = df[df[soc_dist_col].isin([2])][col].values\n",
    "            elif contrast == '1v3':\n",
    "                y = df[df[soc_dist_col].isin([3])][col].values\n",
    "            elif contrast == '1v23':\n",
    "                y = df[df[soc_dist_col].isin([2,3])][col].values\n",
    "\n",
    "            delta = x.mean() - y.mean()\n",
    "            vals[i, j] = delta\n",
    "        \n",
    "    if not os.path.exists(f'{filepath}/derivatives/friend_group_contrast/'):\n",
    "            os.makedirs(f'{filepath}/derivatives/friend_group_contrast/')\n",
    "\n",
    "    df_permuted = pd.DataFrame(vals, columns = cols)\n",
    "    df_permuted.to_csv(f'{filepath}/derivatives/friend_group_contrast/null_{contrast}_control-{control}_v1000_{time}.csv', index = False)\n",
    "\n",
    "def calc_sig(time, contrast, control):\n",
    "    df = master_df\n",
    "\n",
    "    if control == 'demo':\n",
    "        df = regress_out_covariates(df)\n",
    "    if control == 'enjoyment-interest':\n",
    "        df = regress_out_enjoyment_interest(df)\n",
    "\n",
    "    cols = parcels\n",
    "    df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "\n",
    "    if time == 't2':\n",
    "        soc_dist_col = 'soc_dist2'\n",
    "    elif time == 't3':\n",
    "        soc_dist_col = 'soc_dist3'\n",
    "    else:\n",
    "        print('invalid time input')\n",
    "\n",
    "    stats_dict = {}\n",
    "    for col in cols:\n",
    "        x = df[df[soc_dist_col].isin([1])][col].values\n",
    "        if contrast == '1v2':\n",
    "            y = df[df[soc_dist_col].isin([2])][col].values\n",
    "        elif contrast == '1v3':\n",
    "            y = df[df[soc_dist_col].isin([3])][col].values\n",
    "        elif contrast == '1v23':\n",
    "            y = df[df[soc_dist_col].isin([2,3])][col].values\n",
    "        \n",
    "        delta = x.mean() - y.mean()\n",
    "        stats_dict[col] = delta\n",
    "\n",
    "    df_true = pd.DataFrame(stats_dict, index = ['isc_delta']).T\n",
    "\n",
    "    df_permuted = pd.read_csv(f'{filepath}/derivatives/friend_group_contrast/null_{contrast}_control-{control}_v1000_{time}.csv')\n",
    "\n",
    "    rois = cols\n",
    "    dict_true = dict(zip(df_true.index, df_true.isc_delta))\n",
    "\n",
    "    pvals = []\n",
    "    for roi in rois:\n",
    "        true_delta = dict_true[roi]\n",
    "        permuted_deltas = df_permuted[roi].values\n",
    "        pval = (1000 - (permuted_deltas < true_delta).sum()) / 1000\n",
    "        pvals.append(pval)\n",
    "\n",
    "    df_true['pval'] = pvals\n",
    "    fdr_pvals = multitest.fdrcorrection(list(df_true['pval']))[1]\n",
    "    df_true['pval_fdr'] = fdr_pvals\n",
    "    df_true[df_true.pval_fdr < .05]\n",
    "\n",
    "    df_true.sort_values(by='pval_fdr').to_csv(f'{filepath}/derivatives/friend_group_contrast/results_{contrast}_control-{control}_v1000_{time}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the analyses testing if pre-existing neural similarity differed between levels of social distance at Time 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 't3'\n",
    "contrasts = ['1v2', '1v3', '1v23']\n",
    "controls = ['none', 'demo', 'enjoyment-interest']\n",
    "\n",
    "for contrast in contrasts:\n",
    "    for control in controls:\n",
    "        compare_groups_permuted(time, contrast, control)\n",
    "        calc_sig(time, contrast, control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis testing if neural similarity significantly differed as a function of the direction of change in social distance between Time 2 and Time 3\n",
    "Setting up functions compare_groups_permuted_long() and calc_sig_long() to run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groups_permuted_long(contrast, control):\n",
    "    vals = np.zeros([1000, len(parcels)])\n",
    "    for i in range(1000):\n",
    "        df = LoadPickle(f'{filepath}/derivatives/master_dfs_permuted/master_dfs_p{i+1}.pkl')\n",
    "\n",
    "        #calculate the change in social distance from time2 to time3\n",
    "        df['soc_dist_diff'] = df['soc_dist3'] - df['soc_dist2']\n",
    "\n",
    "        if control == 'demo':\n",
    "            df = regress_out_covariates(df)\n",
    "        if control == 'enjoyment-interest':\n",
    "            df = regress_out_enjoyment_interest(df)\n",
    "\n",
    "        cols = parcels\n",
    "        df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "\n",
    "        for j in range(len(cols)):\n",
    "            col = cols[j]\n",
    "\n",
    "            x = df[df['soc_dist_diff'] < 0][col].values # grew closer\n",
    "            if contrast == 'closer_vs_same':\n",
    "                y = df[df['soc_dist_diff'] == 0][col].values # didnt change\n",
    "            elif contrast == 'closer_vs_farther':\n",
    "                y = df[df['soc_dist_diff'] > 0][col].values # grew apart\n",
    "            elif contrast == 'closer_vs_same-farther':\n",
    "                y = df[df['soc_dist_diff'] >= 0][col].values # didnt change or grew apart\n",
    "\n",
    "            delta = x.mean() - y.mean()\n",
    "            vals[i, j] = delta\n",
    "\n",
    "    if not os.path.exists(f'{filepath}/derivatives/dist_change_contrast/'):\n",
    "        os.makedirs(f'{filepath}/derivatives/dist_change_contrast/')\n",
    "\n",
    "    df_permuted = pd.DataFrame(vals, columns = cols)\n",
    "    df_permuted.to_csv(f'{filepath}/derivatives/dist_change_contrast/null_{contrast}_control-{control}_v1000.csv', index = False)\n",
    "\n",
    "def calc_sig_long(contrast, control):\n",
    "    df = master_df\n",
    "\n",
    "    df['soc_dist_diff'] = df['soc_dist3'] - df['soc_dist2']\n",
    "   \n",
    "    if control == 'demo':\n",
    "        df = regress_out_covariates(df)\n",
    "    if control == 'enjoyment-interest':\n",
    "        df = regress_out_enjoyment_interest(df)\n",
    "\n",
    "    cols = parcels\n",
    "    df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "\n",
    "    stats_dict = {}\n",
    "    for col in cols:\n",
    "        x = df[df['soc_dist_diff'] < 0][col].values # grew closer\n",
    "\n",
    "        if contrast == 'closer_vs_same':\n",
    "            y = df[df['soc_dist_diff'] == 0][col].values # didnt change\n",
    "        elif contrast == 'closer_vs_farther':\n",
    "            y = df[df['soc_dist_diff'] > 0][col].values # grew apart\n",
    "        elif contrast == 'closer_vs_same-farther':\n",
    "            y = df[df['soc_dist_diff'] >= 0][col].values # didnt change or grew apart\n",
    "\n",
    "        delta = x.mean() - y.mean()\n",
    "        stats_dict[col] = delta\n",
    "\n",
    "    df_true = pd.DataFrame(stats_dict, index = ['isc_delta']).T\n",
    "\n",
    "    df_permuted = pd.read_csv(f'{filepath}/derivatives/dist_change_contrast/null_{contrast}_control-{control}_v1000.csv')\n",
    "    rois = list(df_permuted.columns)\n",
    "    dict_true = dict(zip(df_true.index, df_true.isc_delta))\n",
    "\n",
    "    pvals = []\n",
    "    for roi in rois:\n",
    "        true_delta = dict_true[roi]\n",
    "        permuted_deltas = df_permuted[roi].values\n",
    "        pval = (1000 - (permuted_deltas < true_delta).sum()) / 1000\n",
    "        pvals.append(pval)\n",
    "\n",
    "    df_true['pval'] = pvals\n",
    "    fdr_pvals = multitest.fdrcorrection(list(df_true['pval']))[1]\n",
    "    df_true['pval_fdr'] = fdr_pvals\n",
    "    df_true[df_true.pval_fdr < .05]\n",
    "\n",
    "    df_true.sort_values(by='pval_fdr').to_csv(f'{filepath}/derivatives/dist_change_contrast/results_{contrast}_control-{control}_v1000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the analysis testing if neural similarity significantly differed as a function of the direction of change in social distance between Time 2 and Time 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_change_contrasts = ['closer_vs_same', 'closer_vs_farther', 'closer_vs_same-farther']\n",
    "controls = ['none', 'demo', 'enjoyment-interest']\n",
    "for contrast in dist_change_contrasts:\n",
    "    for control in controls:\n",
    "        compare_groups_permuted_long(contrast, control)\n",
    "        calc_sig_long(contrast, control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyses testing if inter-individual similarity in self-reported ratings of enjoyment or interest partially but significantly accounted for the significant differences observed in the two sets of main analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1000 permuted dataset where enjoyment and interest ratings were shuffled at the individual level while holding all else in the dataset constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_demo_ratings_permuted = subject_demo_ratings.copy()\n",
    "\n",
    "for perm in range(1000):\n",
    "    shuffle_cols = [interest_cols] + [enjoy_cols]\n",
    "    for shuffle_col in shuffle_cols:\n",
    "        subject_demo_ratings_permuted[shuffle_col] = subject_demo_ratings_permuted[shuffle_col].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #create permuted master dataframe by dropping the \"real\" similarity in enjoyment and interest while holding everything else constant\n",
    "    master_df_preference_permuted = master_df.drop(['enjoy_similarity', 'interest_similarity'], axis = 1)\n",
    "    master_df_preference_permuted['enjoy_similarity'] = ''\n",
    "    master_df_preference_permuted['interest_similarity'] = '' \n",
    "\n",
    "    for i in range(0,len(master_df_preference_permuted)):\n",
    "        dyad_subj1 = master_df_preference_permuted['dyad_subject1'][i]\n",
    "        dyad_subj2 = master_df_preference_permuted['dyad_subject2'][i]\n",
    "\n",
    "        #calculate the enjoyment and interest similarity on the permuted dataset with the enjoyment and interest ratings shuffled\n",
    "        #enjoy_similarity\n",
    "        dyad_subj1_enjoy_vec = stats.zscore(subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject'] == dyad_subj1][enjoy_cols].values.flatten())\n",
    "        dyad_subj2_enjoy_vec = stats.zscore(subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject'] == dyad_subj2][enjoy_cols].values.flatten())\n",
    "        enjoy_distance = np.linalg.norm(dyad_subj1_enjoy_vec - dyad_subj2_enjoy_vec)\n",
    "        master_df_preference_permuted['enjoy_similarity'][i] = float(enjoy_distance)\n",
    "    \n",
    "        #interest_similarity\n",
    "        dyad_subj1_interest_vec = stats.zscore(subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject'] == dyad_subj1][interest_cols].values.flatten())\n",
    "        dyad_subj2_interest_vec = stats.zscore(subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject'] == dyad_subj2][interest_cols].values.flatten())\n",
    "        interest_distance = np.linalg.norm(dyad_subj1_interest_vec - dyad_subj2_interest_vec)\n",
    "        master_df_preference_permuted['interest_similarity'][i] = float(interest_distance)\n",
    "\n",
    "    if not os.path.exists(f'{filepath}/derivatives/master_dfs_preference_permuted/'):\n",
    "        os.makedirs(f'{filepath}/derivatives/master_dfs_preference_permuted/')\n",
    "        \n",
    "    SavePickle(master_df_preference_permuted, f'{filepath}/derivatives/master_dfs_preference_permuted/master_dfs_preference_p{perm+1}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the function to regress out similarity in either enjoyment or interest ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_out_preferences(df, regressor):\n",
    "    regressor_cols = [regressor]\n",
    "    regressors_var = df[regressor_cols]\n",
    "    cols = parcels\n",
    "    df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "    outcome_var = df[cols]\n",
    "\n",
    "    pipe = LinearRegression()\n",
    "    pipe.fit(regressors_var, outcome_var)\n",
    "    predicted = pipe.predict(regressors_var)\n",
    "    actual = outcome_var.values\n",
    "    resid = actual - predicted\n",
    "\n",
    "    resid_df = pd.DataFrame(resid, columns = cols)\n",
    "    df_subset = df[[col for col in df.columns if not col in cols]]\n",
    "    df_final = pd.concat([df_subset, resid_df], axis = 1)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the function to test, for each brain region in which a significance difference in neural similarity between levels of social distance at Time 3 was observed, if inter-individual similarity in self-reported ratings of enjoyment or interest of the stimuli accounted for a significant portion of this difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pref_testing_time3(regressor, contrast):\n",
    "    df1 = master_df\n",
    "    cols = parcels\n",
    "    df1[cols] = StandardScaler().fit_transform(df1[cols])\n",
    "    df2 = regress_out_preferences(df1, regressor = regressor)\n",
    "\n",
    "    soc_dist = 'soc_dist3'\n",
    "\n",
    "    dd_dict = {}\n",
    "    for col in cols:\n",
    "        x1 = df1[df1[soc_dist].isin([1])][col].values\n",
    "        x2 = df2[df2[soc_dist].isin([1])][col].values\n",
    "\n",
    "        if contrast == '1v2':\n",
    "            y1 = df1[df1[soc_dist].isin([2])][col].values\n",
    "            y2 = df2[df2[soc_dist].isin([2])][col].values\n",
    "        elif contrast == '1v3':\n",
    "            y1 = df1[df1[soc_dist].isin([3])][col].values\n",
    "            y2 = df2[df2[soc_dist].isin([3])][col].values\n",
    "        elif contrast == '1v23':\n",
    "            y1 = df1[df1[soc_dist].isin([2,3])][col].values\n",
    "            y2 = df2[df2[soc_dist].isin([2,3])][col].values\n",
    "\n",
    "        #calculate delta 1, which is the difference in ISC (the contrast)\n",
    "        delta1 = x1.mean() - y1.mean()\n",
    "        #calculate delta 2, which is the difference in ISC (the contrast), controlling for the preference variable (enjoyment or preference)\n",
    "        delta2 = x2.mean() - y2.mean()\n",
    "\n",
    "        #calculate dd, which is the extent to which the 'uncontrolled' ISC difference is greater than the 'controlled' ISC difference\n",
    "        #thus, dd captures the extent to which the ISC difference is reduced when controlling for the preference variable\n",
    "        #thereby capturing the extent to the preference variable might account for the ISC difference\n",
    "        dd = delta1 - delta2\n",
    "\n",
    "        dd_dict[col] = dd\n",
    "\n",
    "    df_dd = pd.DataFrame(dd_dict, index = ['dd']).T\n",
    "\n",
    "    # Permutation testing, repeating the procedure above but using preferences shuffled at the individual level\n",
    "    vals = np.zeros([1000, len(parcels)])\n",
    "    for i in range(1000):\n",
    "        df1 = LoadPickle(f'{filepath}/derivatives/master_dfs_preference_permuted/master_dfs_preference_p{i+1}.pkl')\n",
    "        df1[cols] = StandardScaler().fit_transform(df1[cols])\n",
    "        df2 = regress_out_preferences(df1, regressor = regressor)\n",
    "\n",
    "        soc_dist = 'soc_dist3'\n",
    "\n",
    "        for j in range(len(cols)):\n",
    "            col = cols[j]\n",
    "\n",
    "            x1 = df1[df1[soc_dist].isin([1])][col].values\n",
    "            x2 = df2[df2[soc_dist].isin([1])][col].values\n",
    "\n",
    "            if contrast == '1v2':\n",
    "                y1 = df1[df1[soc_dist].isin([2])][col].values\n",
    "                y2 = df2[df2[soc_dist].isin([2])][col].values\n",
    "            elif contrast == '1v3':\n",
    "                y1 = df1[df1[soc_dist].isin([3])][col].values\n",
    "                y2 = df2[df2[soc_dist].isin([3])][col].values\n",
    "            elif contrast == '1v23':\n",
    "                y1 = df1[df1[soc_dist].isin([2,3])][col].values\n",
    "                y2 = df2[df2[soc_dist].isin([2,3])][col].values\n",
    "\n",
    "            delta1 = x1.mean() - y1.mean()\n",
    "            delta2 = x2.mean() - y2.mean()\n",
    "            dd = delta1 - delta2\n",
    "\n",
    "            vals[i, j] = dd\n",
    "    df_permuted = pd.DataFrame(vals, columns = cols)\n",
    "    dict_true = dict(zip(df_dd.index, df_dd.dd))\n",
    "\n",
    "    pvals = []\n",
    "    for roi in cols:\n",
    "        true_delta = dict_true[roi]\n",
    "        permuted_deltas = df_permuted[roi].values\n",
    "        pval = (1000 - (permuted_deltas < true_delta).sum()) / 1000\n",
    "        pvals.append(pval)\n",
    "\n",
    "    df_dd['pval'] = pvals\n",
    "    df_dd.sort_values(by='pval').to_csv(f'{filepath}/derivatives/friend_group_contrast/{contrast}_{regressor}-DoD_v1000_t3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if inter-individual similarity in self-reported ratings of enjoyment or interest of the stimuli accounted for the observed difference in pre-existing neural similarity between friends (with a social distance of 1) versus friends-of-friends-of-friends (with a social distance of 3) in all brain parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast = '1v3' #where significant differences in pre-existing neural similarity was observed in the source data\n",
    "\n",
    "for regressor in ['enjoy_similarity', 'interest_similarity']:\n",
    "    pref_testing_time3(regressor, contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether brain parcels in which a significance difference in neural similarity between levels of social distance at Time 3 was observed include any parcel in which either enjoyment or interest significantly accounts for the ISC difference (if the output is an empty dataframe, then this suggests that enjoyment/interest ratings do not significantly account for the significant ISC difference observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoy_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "interest_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def check_pref_testing_time3(regressor, contrast):\n",
    "    df = pd.read_csv(f'{filepath}/derivatives/friend_group_contrast/results_{contrast}_control-none_v1000_t3.csv')\n",
    "    pref = pd.read_csv(f'{filepath}/derivatives/friend_group_contrast/{contrast}_{regressor}-DoD_v1000_t3.csv')\n",
    "    \n",
    "    #get significant brain parcels\n",
    "    sig_rois = [_ for _ in list(df[df['pval_fdr'] < .05]['Unnamed: 0'])]\n",
    "    pref_sig = pref[pref['pval'] < .05]\n",
    "\n",
    "    foo = pref_sig[pref_sig['Unnamed: 0'].isin(sig_rois)]\n",
    "    return foo\n",
    "\n",
    "for regressor in ['enjoy_similarity', 'interest_similarity']:\n",
    "    contrast = '1v3'\n",
    "    print(f'{regressor}_{contrast}:')\n",
    "    print(check_pref_testing_time3(regressor, contrast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the function to test, for each brain region in which a significance difference in neural similarity between levels of social distance change from Time 2 to Time 3 was observed, if inter-individual similarity in self-reported ratings of enjoyment or interest of the stimuli accounted for a significant portion of this difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pref_testing_dist_change(regressor, contrast):\n",
    "    df1 = master_df\n",
    "    cols = parcels\n",
    "    df1[cols] = StandardScaler().fit_transform(df1[cols])\n",
    "    df2 = regress_out_preferences(df1, regressor = regressor)\n",
    "\n",
    "    dd_dict = {}\n",
    "    for col in cols:\n",
    "        x1 = df1[df1['soc_dist_diff'] < 0][col].values\n",
    "        x2 = df2[df2['soc_dist_diff'] < 0][col].values\n",
    "\n",
    "        if contrast == 'closer_vs_same':\n",
    "            y1 = df1[df1['soc_dist_diff'] == 0][col].values\n",
    "            y2 = df2[df2['soc_dist_diff'] == 0][col].values\n",
    "        elif contrast == 'closer_vs_farther':\n",
    "            y1 = df1[df1['soc_dist_diff'] > 0][col].values\n",
    "            y2 = df2[df2['soc_dist_diff'] > 0][col].values\n",
    "        elif contrast == 'closer_vs_same-farther':\n",
    "            y1 = df1[df1['soc_dist_diff'] >= 0][col].values\n",
    "            y2 = df2[df2['soc_dist_diff'] >= 0][col].values\n",
    "\n",
    "        #calculate delta 1, which is the difference in ISC (the contrast)\n",
    "        delta1 = x1.mean() - y1.mean()\n",
    "        #calculate delta 2, which is the difference in ISC (the contrast), controlling for the preference variable (enjoyment or preference)\n",
    "        delta2 = x2.mean() - y2.mean()\n",
    "\n",
    "        #calculate dd, which is the extent to which the 'uncontrolled' ISC difference is greater than the 'controlled' ISC difference\n",
    "        #thus, dd captures the extent to which the ISC difference is reduced when controlling for the preference variable\n",
    "        #thereby capturing the extent to the preference variable might account for the ISC difference\n",
    "        dd = delta1 - delta2\n",
    "\n",
    "        dd_dict[col] = dd\n",
    "\n",
    "    df_dd = pd.DataFrame(dd_dict, index = ['dd']).T\n",
    "\n",
    "    # Permutation testing, repeating the procedure above but using preferences shuffled at the individual level\n",
    "    vals = np.zeros([1000, len(parcels)])\n",
    "    for i in range(1000):\n",
    "        df1 = LoadPickle(f'{filepath}/derivatives/master_dfs_preference_permuted/master_dfs_preference_p{i+1}.pkl')\n",
    "        df1[cols] = StandardScaler().fit_transform(df1[cols])\n",
    "        df2 = regress_out_preferences(df1, regressor = regressor)\n",
    "\n",
    "        for j in range(len(cols)):\n",
    "            col = cols[j]\n",
    "\n",
    "            x1 = df1[df1['soc_dist_diff'] < 0][col].values\n",
    "            x2 = df2[df2['soc_dist_diff'] < 0][col].values\n",
    "\n",
    "            if contrast == 'closer_vs_same':\n",
    "                y1 = df1[df1['soc_dist_diff'] == 0][col].values\n",
    "                y2 = df2[df2['soc_dist_diff'] == 0][col].values\n",
    "            elif contrast == 'closer_vs_farther':\n",
    "                y1 = df1[df1['soc_dist_diff'] > 0][col].values\n",
    "                y2 = df2[df2['soc_dist_diff'] > 0][col].values\n",
    "            elif contrast == 'closer_vs_same-farther':\n",
    "                y1 = df1[df1['soc_dist_diff'] >= 0][col].values\n",
    "                y2 = df2[df2['soc_dist_diff'] >= 0][col].values\n",
    "\n",
    "            delta1 = x1.mean() - y1.mean()\n",
    "            delta2 = x2.mean() - y2.mean()\n",
    "            dd = delta1 - delta2\n",
    "\n",
    "            vals[i, j] = dd\n",
    "    df_permuted = pd.DataFrame(vals, columns = cols)\n",
    "    dict_true = dict(zip(df_dd.index, df_dd.dd))\n",
    "\n",
    "    pvals = []\n",
    "    for roi in cols:\n",
    "        true_delta = dict_true[roi]\n",
    "        permuted_deltas = df_permuted[roi].values\n",
    "        pval = (1000 - (permuted_deltas < true_delta).sum()) / 1000\n",
    "        pvals.append(pval)\n",
    "\n",
    "    df_dd['pval'] = pvals\n",
    "    df_dd.sort_values(by='pval').to_csv(f'{filepath}/derivatives/dist_change_contrast/{contrast}_{regressor}-DoD_v1000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if inter-individual similarity in self-reported ratings of enjoyment or interest of the stimuli accounted for the observed difference in pre-existing neural similarity between dyads who grew closer versus those who grew apart in all brain parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast = 'closer_vs_farther' #where significant differences in pre-existing neural similarity was observed in the source data\n",
    "\n",
    "for regressor in ['enjoy_similarity', 'interest_similarity']:\n",
    "    pref_testing_dist_change(regressor, contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether brain parcels in which a significance difference in neural similarity between direction of change in social distance between Time 2 and Time 3 was observed include any parcel in which either enjoyment or interest significantly accounts for the ISC difference (if the output is an empty dataframe, then this suggests that enjoyment/interest ratings do not significantly account for the significant ISC difference observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoy_similarity_closer_vs_farther:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "interest_similarity_closer_vs_farther:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def check_pref_testing_dist_change(regressor, contrast):\n",
    "    df = pd.read_csv(f'{filepath}/derivatives/dist_change_contrast/results_{contrast}_control-none_v1000.csv')\n",
    "    pref = pd.read_csv(f'{filepath}/derivatives/dist_change_contrast/{contrast}_{regressor}-DoD_v1000.csv')\n",
    "    \n",
    "    #get significant brain parcels\n",
    "    sig_rois = [_ for _ in list(df[df['pval_fdr'] < .05]['Unnamed: 0'])]\n",
    "    pref_sig = pref[pref['pval'] < .05]\n",
    "\n",
    "    foo = pref_sig[pref_sig['Unnamed: 0'].isin(sig_rois)]\n",
    "    return foo\n",
    "\n",
    "contrast = 'closer_vs_farther'\n",
    "\n",
    "for regressor in ['enjoy_similarity', 'interest_similarity']:\n",
    "    print(f'{regressor}_{contrast}:')\n",
    "    print(check_pref_testing_dist_change(regressor, contrast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyses testing if inter-individual similarity in sociodemographic variables partially but significantly accounted for the significant differences in pre-existing neural similarity observed in the first set of main analyses (friends vs friends-of-friends-of-friends) at Time 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1000 permuted dataset where all sociodemographic ratings were shuffled at the individual level while holding all else in the dataset constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_demo_ratings_permuted = subject_demo_ratings.copy()\n",
    "\n",
    "for perm in range(1000):\n",
    "    shuffle_cols = ['age', 'gender', 'nationality', 'college', 'hometown', 'major', 'industry']\n",
    "    for shuffle_col in shuffle_cols:\n",
    "        subject_demo_ratings_permuted[shuffle_col] = subject_demo_ratings_permuted[shuffle_col].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #create permuted master dataframe by dropping the \"real\" similarity in enjoyment and interest while holding everything else constant\n",
    "    master_df_demo_permuted = master_df.drop(['age_dist', 'gender_similarity', 'nationality_similarity', 'hometown_population_similarity', 'dist_hometown','dist_college', 'college_pub_priv_similarity', 'major_similarity', 'industry_similarity'], axis = 1)\n",
    "    master_df_demo_permuted['age_dist'] = ''\n",
    "    master_df_demo_permuted['gender_similarity'] = '' \n",
    "    master_df_demo_permuted['nationality_similarity'] = '' \n",
    "    master_df_demo_permuted['hometown_population_similarity'] = '' \n",
    "    master_df_demo_permuted['dist_hometown'] = '' \n",
    "    master_df_demo_permuted['dist_college'] = '' \n",
    "    master_df_demo_permuted['college_pub_priv_similarity'] = '' \n",
    "    master_df_demo_permuted['major_similarity'] = '' \n",
    "    master_df_demo_permuted['industry_similarity'] = '' \n",
    "\n",
    "    for i in range(0,len(master_df_demo_permuted)):\n",
    "        dyad_subj1 = master_df_demo_permuted['dyad_subject1'][i]\n",
    "        dyad_subj2 = master_df_demo_permuted['dyad_subject2'][i]\n",
    "\n",
    "        #calculate the enjoyment and interest similarity on the permuted dataset with the demographic variables shuffled\n",
    "        #age_dist\n",
    "        dyad_subj1_age = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['age'].item()\n",
    "        dyad_subj2_age = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['age'].item()\n",
    "        age_dist = np.abs(dyad_subj1_age - dyad_subj2_age)\n",
    "        master_df_demo_permuted['age_dist'][i] = age_dist\n",
    "\n",
    "        #gender_similarity\n",
    "        dyad_subj1_gender = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['gender'].item()\n",
    "        dyad_subj2_gender = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['gender'].item()\n",
    "        gender_similarity = 1 if dyad_subj1_gender == dyad_subj2_gender else 0\n",
    "        master_df_demo_permuted['gender_similarity'][i] = gender_similarity\n",
    "        \n",
    "        #nationality_similarity\n",
    "        dyad_subj1_nationality = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['nationality'].item()\n",
    "        dyad_subj2_nationality = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['nationality'].item()\n",
    "        nationality_similarity = 1 if dyad_subj1_nationality == dyad_subj2_nationality else 0\n",
    "        master_df_demo_permuted['nationality_similarity'][i] = nationality_similarity\n",
    "\n",
    "        #hometown_population_similarity\n",
    "        dyad_subj1_hometown = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['hometown'].item()\n",
    "        dyad_subj2_hometown = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['hometown'].item()\n",
    "        hometown_population_similarity = np.abs(dict_hometown_population[dyad_subj1_hometown] - dict_hometown_population[dyad_subj2_hometown])\n",
    "        master_df_demo_permuted['hometown_population_similarity'][i] = hometown_population_similarity\n",
    "\n",
    "        #dist_hometown\n",
    "        if dyad_subj1_hometown == dyad_subj2_hometown:\n",
    "            master_df_demo_permuted['dist_hometown'][i] = 0\n",
    "        else: \n",
    "            hometown_idx = dist_hometowns.index[(dist_hometowns['City1'] == dyad_subj1_hometown) & (dist_hometowns['City2'] == dyad_subj2_hometown)].tolist()\n",
    "            if not hometown_idx:\n",
    "                hometown_idx = dist_hometowns.index[(dist_hometowns['City2'] == dyad_subj1_hometown) & (dist_hometowns['City1'] == dyad_subj2_hometown)].tolist()\n",
    "            master_df_demo_permuted['dist_hometown'][i] = dist_hometowns.loc[hometown_idx[0], 'dist_hometown']\n",
    "\n",
    "        #dist_college\n",
    "        dyad_subj1_college = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['college'].item()\n",
    "        dyad_subj2_college = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['college'].item()\n",
    "\n",
    "        if dyad_subj1_college == dyad_subj2_college:\n",
    "            master_df_demo_permuted['dist_college'][i] = 0\n",
    "        else:\n",
    "            college_idx = dist_colleges.index[(dist_colleges['college1'] == dyad_subj1_college) & (dist_colleges['college2'] == dyad_subj2_college)].tolist()\n",
    "            if not college_idx:\n",
    "                college_idx = dist_colleges.index[(dist_colleges['college2'] == dyad_subj1_college) & (dist_colleges['college1'] == dyad_subj2_college)].tolist()\n",
    "            master_df_demo_permuted['dist_college'][i] = dist_colleges.loc[college_idx[0], 'dist_college']\n",
    "\n",
    "        #college_pub_priv_similarity\n",
    "        college_pub_priv_similarity = 1 if dict_college_public_private[dyad_subj1_college] == dict_college_public_private[dyad_subj2_college] else 0\n",
    "        master_df_demo_permuted['college_pub_priv_similarity'][i] = college_pub_priv_similarity\n",
    "\n",
    "        #major_similarity\n",
    "        dyad_subj1_major = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['major'].item()\n",
    "        dyad_subj2_major = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['major'].item()\n",
    "        major_similarity = 1 if dict_major_cat[dyad_subj1_major] == dict_major_cat[dyad_subj2_major] else 0\n",
    "        master_df_demo_permuted['major_similarity'][i] = major_similarity\n",
    "\n",
    "        #industry_similarity\n",
    "        dyad_subj1_industry = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj1]['industry'].item()\n",
    "        dyad_subj2_industry = subject_demo_ratings_permuted[subject_demo_ratings_permuted['subject']==dyad_subj2]['industry'].item()\n",
    "        industry_similarity = 1 if dyad_subj1_industry == dyad_subj2_industry else 0\n",
    "        master_df_demo_permuted['industry_similarity'][i] = industry_similarity\n",
    "\n",
    "    if not os.path.exists(f'{filepath}/derivatives/master_dfs_demo_permuted/'):\n",
    "        os.makedirs(f'{filepath}/derivatives/master_dfs_demo_permuted/')\n",
    "        \n",
    "    SavePickle(master_df_demo_permuted, f'{filepath}/derivatives/master_dfs_demo_permuted/master_dfs_demo_p{perm+1}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the function to test, for each brain region in which a significance difference in neural similarity between levels of social distance at Time 3 was observed, if inter-individual similarity in all or any demographic variables of the stimuli accounted for a significant portion of this difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_out_demo(df, regressor_cols):\n",
    "    regressors_var = df[regressor_cols]\n",
    "    cols = parcels\n",
    "    df[cols] = StandardScaler().fit_transform(df[cols])\n",
    "    outcome_var = df[cols]\n",
    "\n",
    "    pipe = LinearRegression()\n",
    "    pipe.fit(regressors_var, outcome_var)\n",
    "    predicted = pipe.predict(regressors_var)\n",
    "    actual = outcome_var.values\n",
    "    resid = actual - predicted\n",
    "\n",
    "    resid_df = pd.DataFrame(resid, columns = cols)\n",
    "    df_subset = df[[col for col in df.columns if not col in cols]]\n",
    "    df_final = pd.concat([df_subset, resid_df], axis = 1)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def demo_testing_time3(regressor, contrast):\n",
    "    df1 = master_df\n",
    "    cols = parcels\n",
    "    df1[cols] = StandardScaler().fit_transform(df1[cols])\n",
    "    df2 = regress_out_demo(df1, regressor)\n",
    "\n",
    "    soc_dist = 'soc_dist3'\n",
    "\n",
    "    dd_dict = {}\n",
    "    for col in cols:\n",
    "        x1 = df1[df1[soc_dist].isin([1])][col].values\n",
    "        x2 = df2[df2[soc_dist].isin([1])][col].values\n",
    "\n",
    "        if contrast == '1v2':\n",
    "            y1 = df1[df1[soc_dist].isin([2])][col].values\n",
    "            y2 = df2[df2[soc_dist].isin([2])][col].values\n",
    "        elif contrast == '1v3':\n",
    "            y1 = df1[df1[soc_dist].isin([3])][col].values\n",
    "            y2 = df2[df2[soc_dist].isin([3])][col].values\n",
    "        elif contrast == '1v23':\n",
    "            y1 = df1[df1[soc_dist].isin([2,3])][col].values\n",
    "            y2 = df2[df2[soc_dist].isin([2,3])][col].values\n",
    "\n",
    "        delta1 = x1.mean() - y1.mean()\n",
    "        delta2 = x2.mean() - y2.mean()\n",
    "\n",
    "        # delta 1 is the difference in ISC (the contrast)\n",
    "        # delta 2 is the difference in ISC (the contrast), controlling for the demographic variable(s)\n",
    "        # dd is the extent to which the 'uncontrolled' ISC difference is greater than the 'controlled' ISC difference\n",
    "        # thus, dd captures the extent to which the ISC difference is reduced when controlling for the demographic variable(s)\n",
    "        # thereby capturing the extent to the demographic variable(s) might account for the ISC difference\n",
    "        dd = delta1 - delta2\n",
    "\n",
    "        dd_dict[col] = dd\n",
    "\n",
    "    df_dd = pd.DataFrame(dd_dict, index = ['dd']).T\n",
    "\n",
    "    # Permutation testing, repeating the procedure above but using demographic variables shuffled at the individual level\n",
    "    vals = np.zeros([1000, len(parcels)])\n",
    "    for i in range(1000):\n",
    "        df1 = LoadPickle(f'{filepath}/derivatives/master_dfs_demo_permuted/master_dfs_demo_p{i+1}.pkl')\n",
    "        df1[cols] = StandardScaler().fit_transform(df1[cols])\n",
    "        df2 = regress_out_demo(df1, regressor)\n",
    "\n",
    "        soc_dist = 'soc_dist3'\n",
    "\n",
    "        for j in range(len(cols)):\n",
    "            col = cols[j]\n",
    "\n",
    "            x1 = df1[df1[soc_dist].isin([1])][col].values\n",
    "            x2 = df2[df2[soc_dist].isin([1])][col].values\n",
    "\n",
    "            if contrast == '1v2':\n",
    "                y1 = df1[df1[soc_dist].isin([2])][col].values\n",
    "                y2 = df2[df2[soc_dist].isin([2])][col].values\n",
    "            elif contrast == '1v3':\n",
    "                y1 = df1[df1[soc_dist].isin([3])][col].values\n",
    "                y2 = df2[df2[soc_dist].isin([3])][col].values\n",
    "            elif contrast == '1v23':\n",
    "                y1 = df1[df1[soc_dist].isin([2,3])][col].values\n",
    "                y2 = df2[df2[soc_dist].isin([2,3])][col].values\n",
    "\n",
    "            delta1 = x1.mean() - y1.mean()\n",
    "            delta2 = x2.mean() - y2.mean()\n",
    "            dd = delta1 - delta2\n",
    "\n",
    "            vals[i, j] = dd\n",
    "    df_permuted = pd.DataFrame(vals, columns = cols)\n",
    "    dict_true = dict(zip(df_dd.index, df_dd.dd))\n",
    "\n",
    "    pvals = []\n",
    "    for roi in cols:\n",
    "        true_delta = dict_true[roi]\n",
    "        permuted_deltas = df_permuted[roi].values\n",
    "        pval = (1000 - (permuted_deltas < true_delta).sum()) / 1000\n",
    "        pvals.append(pval)\n",
    "\n",
    "    df_dd['pval'] = pvals\n",
    "    return df_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if inter-individual similarity in demographic variable(s) accounted for the observed difference in pre-existing neural similarity between friends (with a social distance of 1) versus friends-of-friends-of-friends (with a social distance of 3) in all brain parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of demographic variables as regressors \n",
    "demo_regressor_cols = ['age_dist', 'gender_similarity', 'nationality_similarity', 'dist_hometown','dist_college', 'college_pub_priv_similarity', 'major_similarity', 'hometown_population_similarity', 'industry_similarity']\n",
    "\n",
    "contrast = '1v3' #where significant differences in pre-existing neural similarity was observed in the source data\n",
    "\n",
    "# Check for all regressors (altogether)\n",
    "demo_all_DoD = demo_testing_time3(demo_regressor_cols, contrast) \n",
    "demo_all_DoD.sort_values(by='pval').to_csv(f'{filepath}/derivatives/friend_group_contrast/{contrast}_demo-all-DoD_v1000_t3.csv')\n",
    "\n",
    "# Check for individual regressor\n",
    "for demo_regressor_col in demo_regressor_cols:\n",
    "    demo_regressor_col1 = [demo_regressor_col]\n",
    "    demo_DoD = demo_testing_time3(demo_regressor_col1, contrast)\n",
    "    demo_DoD.sort_values(by='pval').to_csv(f'{filepath}/derivatives/friend_group_contrast/{contrast}_{demo_regressor_col}-DoD_v1000_t3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether brain parcels in which a significance difference in neural similarity between levels of social distance at Time 3 was observed include any parcel in which demographic variable(s) account(s) for the ISC difference (if the output is an empty dataframe, then this suggests that enjoyment/interest ratings do not significantly account for the significant ISC difference observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_dist_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "gender_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "nationality_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "dist_hometown_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "dist_college_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "college_pub_priv_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "major_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "hometown_population_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "industry_similarity_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n",
      "demo-all_1v3:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, dd, pval]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def check_demo_testing_time3(regressor, contrast):\n",
    "    df = pd.read_csv(f'{filepath}/derivatives/friend_group_contrast/results_{contrast}_control-none_v1000_t3.csv')\n",
    "    demo = pd.read_csv(f'{filepath}/derivatives/friend_group_contrast/{contrast}_{regressor}-DoD_v1000_t3.csv')\n",
    "    \n",
    "    #get significant brain parcels\n",
    "    sig_rois = [_ for _ in list(df[df['pval_fdr'] < .05]['Unnamed: 0'])]\n",
    "    demo_sig = demo[demo['pval'] < .05]\n",
    "\n",
    "    foo = demo_sig[demo_sig['Unnamed: 0'].isin(sig_rois)]\n",
    "    return foo\n",
    "\n",
    "for regressor in demo_regressor_cols + ['demo-all']:\n",
    "    contrast = '1v3'\n",
    "    print(f'{regressor}_{contrast}:')\n",
    "    print(check_pref_testing_time3(regressor, contrast))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
